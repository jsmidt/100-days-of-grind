{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from time import sleep\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2217: RuntimeWarning: coroutine 'Agent.run' was never awaited\n",
      "  def __init__(self, name, parent):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "John Doe\n",
      "John Doe\n",
      "John Doe\n",
      "John Doe\n",
      "Get me foobar.\n",
      "{'properties': {'a': {'description': 'apple pie', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'banana cake', 'title': 'B', 'type': 'string'}, 'c': {'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'}, 'description': 'carrot smoothie', 'title': 'C', 'type': 'object'}}, 'required': ['a', 'b', 'c'], 'type': 'object', 'additionalProperties': False}\n",
      "Get me foobar.\n",
      "{'properties': {'a': {'description': 'apple pie', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'banana cake', 'title': 'B', 'type': 'string'}, 'c': {'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'}, 'description': 'carrot smoothie', 'title': 'C', 'type': 'object'}}, 'required': ['a', 'b', 'c'], 'type': 'object', 'additionalProperties': False}\n",
      "Get me foobar.\n",
      "{'properties': {'a': {'description': 'apple pie', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'banana cake', 'title': 'B', 'type': 'string'}, 'c': {'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'}, 'description': 'carrot smoothie', 'title': 'C', 'type': 'object'}}, 'required': ['a', 'b', 'c'], 'type': 'object', 'additionalProperties': False}\n",
      "Get me foobar.\n",
      "{'properties': {'a': {'description': 'apple pie', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'banana cake', 'title': 'B', 'type': 'string'}, 'c': {'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'}, 'description': 'carrot smoothie', 'title': 'C', 'type': 'object'}}, 'required': ['a', 'b', 'c'], 'type': 'object', 'additionalProperties': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roulette_agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=int,\n",
    "    result_type=bool,\n",
    "    system_prompt=(\n",
    "        'Use the `roulette_wheel` function to see if the '\n",
    "        'customer has won based on the number they provide.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@roulette_agent.tool\n",
    "async def roulette_wheel(ctx: RunContext[int], square: int) -> str:\n",
    "    \"\"\"check if the square is a winner\"\"\"\n",
    "    return 'winner' if square == ctx.deps else 'loser'\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "success_number = 18\n",
    "result = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\n",
    "print(result.data)\n",
    "#> True\n",
    "\n",
    "result = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\n",
    "print(result.data)\n",
    "#> False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome\n",
      "\n",
      "Paris\n",
      "\n",
      "London\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash')\n",
    "\n",
    "result_sync = agent.run_sync('What is the capital of Italy?')\n",
    "print(result_sync.data)\n",
    "#> Rome\n",
    "\n",
    "\n",
    "#sync def main():\n",
    "result = await agent.run('What is the capital of France?')\n",
    "print(result.data)\n",
    "#> Paris\n",
    "\n",
    "async with agent.run_stream('What is the capital of the UK?') as response:\n",
    "    print(await response.get_data())\n",
    "    #> London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome\n",
      "\n",
      "Usage(requests=1, request_tokens=13, response_tokens=2, total_tokens=15, details=None)\n",
      "Exceeded the response_tokens_limit of 10 (response_tokens=73)\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.exceptions import UsageLimitExceeded\n",
    "from pydantic_ai.usage import UsageLimits\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash')\n",
    "\n",
    "result_sync = agent.run_sync(\n",
    "    'What is the capital of Italy? Answer with just the city.',\n",
    "    usage_limits=UsageLimits(response_tokens_limit=10),\n",
    ")\n",
    "print(result_sync.data)\n",
    "#> Rome\n",
    "print(result_sync.usage())\n",
    "\"\"\"\n",
    "Usage(requests=1, request_tokens=62, response_tokens=1, total_tokens=63, details=None)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    result_sync = agent.run_sync(\n",
    "        'What is the capital of Italy? Answer with a paragraph.',\n",
    "        usage_limits=UsageLimits(response_tokens_limit=10),\n",
    "    )\n",
    "except UsageLimitExceeded as e:\n",
    "    print(e)\n",
    "    #> Exceeded the response_tokens_limit of 10 (response_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next request would exceed the request_limit of 3\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry\n",
    "from pydantic_ai.exceptions import UsageLimitExceeded\n",
    "from pydantic_ai.usage import UsageLimits\n",
    "\n",
    "\n",
    "class NeverResultType(TypedDict):\n",
    "    \"\"\"\n",
    "    Never ever coerce data to this type.\n",
    "    \"\"\"\n",
    "\n",
    "    never_use_this: str\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    retries=3,\n",
    "    result_type=NeverResultType,\n",
    "    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n",
    ")\n",
    "\n",
    "\n",
    "@agent.tool_plain(retries=5)  \n",
    "def infinite_retry_tool() -> int:\n",
    "    raise ModelRetry('Please try again.')\n",
    "\n",
    "\n",
    "try:\n",
    "    result_sync = agent.run_sync(\n",
    "        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  \n",
    "    )\n",
    "except UsageLimitExceeded as e:\n",
    "    print(e)\n",
    "    #> The next request would exceed the request_limit of 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'never_use_this': 'Rome'}\n"
     ]
    }
   ],
   "source": [
    "result_sync = agent.run_sync(\n",
    "    'What is the capital of Italy?', model_settings={'temperature': 1.0}\n",
    ")\n",
    "print(result_sync.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety settings triggered, body:\n",
      "{'candidates': [{'finish_reason': 'SAFETY', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'LOW', 'blocked': True}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}]}], 'usage_metadata': {'prompt_token_count': 25, 'total_token_count': 25}, 'model_version': 'gemini-1.5-flash'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, UnexpectedModelBehavior\n",
    "from pydantic_ai.models.gemini import GeminiModelSettings\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash')\n",
    "\n",
    "try:\n",
    "    result = agent.run_sync(\n",
    "        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n",
    "        model_settings=GeminiModelSettings(\n",
    "            temperature=0.0,  # general model settings can also be specified\n",
    "            gemini_safety_settings=[\n",
    "                {\n",
    "                    'category': 'HARM_CATEGORY_HARASSMENT',\n",
    "                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n",
    "                },\n",
    "                {\n",
    "                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
    "                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n",
    "                },\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "except UnexpectedModelBehavior as e:\n",
    "    print(e)  \n",
    "    \"\"\"\n",
    "    Safety settings triggered, body:\n",
    "    <safety settings details>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Albert Einstein (1879-1955) was a German-born theoretical physicist who developed the theory of relativity, one of \n",
       "the two pillars of modern physics (alongside quantum mechanics).  His work is also known for its influence on the  \n",
       "philosophy of science.                                                                                             \n",
       "\n",
       "He is best known for his mass–energy equivalence formula  E=mc², which has been dubbed \"the world's most famous    \n",
       "equation\".  His contributions to physics include:                                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">The Special Theory of Relativity:</span> This theory revolutionized our understanding of space, time, gravity, and the \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>universe at large, showing that space and time are relative and interconnected.                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">The General Theory of Relativity:</span>  An extension of the special theory, this theory provided a new understanding \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>of gravity as a curvature of spacetime caused by mass and energy.  It predicted phenomena like gravitational    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>lensing and gravitational waves, which have since been observed.                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">The Photoelectric Effect:</span> His explanation of this phenomenon, which describes how light can knock electrons off \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a metal surface, earned him the Nobel Prize in Physics in 1921.  This work was crucial for the development of   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>quantum mechanics.                                                                                              \n",
       "\n",
       "Beyond his scientific achievements, Einstein was a humanist and pacifist who was deeply concerned about social     \n",
       "justice and the potential dangers of unchecked technological advancement. He became a cultural icon, representing  \n",
       "genius and intellectual curiosity.  He is considered one of the most influential scientists of all time.           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Albert Einstein (1879-1955) was a German-born theoretical physicist who developed the theory of relativity, one of \n",
       "the two pillars of modern physics (alongside quantum mechanics).  His work is also known for its influence on the  \n",
       "philosophy of science.                                                                                             \n",
       "\n",
       "He is best known for his mass–energy equivalence formula  E=mc², which has been dubbed \"the world's most famous    \n",
       "equation\".  His contributions to physics include:                                                                  \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mThe Special Theory of Relativity:\u001b[0m This theory revolutionized our understanding of space, time, gravity, and the \n",
       "\u001b[1;33m   \u001b[0muniverse at large, showing that space and time are relative and interconnected.                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mThe General Theory of Relativity:\u001b[0m  An extension of the special theory, this theory provided a new understanding \n",
       "\u001b[1;33m   \u001b[0mof gravity as a curvature of spacetime caused by mass and energy.  It predicted phenomena like gravitational    \n",
       "\u001b[1;33m   \u001b[0mlensing and gravitational waves, which have since been observed.                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mThe Photoelectric Effect:\u001b[0m His explanation of this phenomenon, which describes how light can knock electrons off \n",
       "\u001b[1;33m   \u001b[0ma metal surface, earned him the Nobel Prize in Physics in 1921.  This work was crucial for the development of   \n",
       "\u001b[1;33m   \u001b[0mquantum mechanics.                                                                                              \n",
       "\n",
       "Beyond his scientific achievements, Einstein was a humanist and pacifist who was deeply concerned about social     \n",
       "justice and the potential dangers of unchecked technological advancement. He became a cultural icon, representing  \n",
       "genius and intellectual curiosity.  He is considered one of the most influential scientists of all time.           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">His most famous equation is <span style=\"font-weight: bold\">E=mc²</span>.                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "His most famous equation is \u001b[1mE=mc²\u001b[0m.                                                                                 \n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich.markdown import Markdown\n",
    "# First run\n",
    "result1 = agent.run_sync('Who was Albert Einstein?')\n",
    "display(Markdown(result1.data))\n",
    "#> Albert Einstein was a German-born theoretical physicist.\n",
    "\n",
    "# Second run, passing previous messages\n",
    "result2 = agent.run_sync(\n",
    "    'What was his most famous equation?',\n",
    "    message_history=result1.new_messages(),  \n",
    ")\n",
    "Markdown(result2.data)\n",
    "#> Albert Einstein's most famous equation is (E = mc^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    name: str\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=User,  \n",
    "    result_type=bool,\n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "def add_user_name(ctx: RunContext[str]) -> str:  \n",
    "    return f\"The user's name is {ctx.deps}.\"\n",
    "\n",
    "\n",
    "def foobar(x: bytes) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "result = agent.run_sync('Does their name start with \"A\"?', deps=User('Bnne'))\n",
    "result.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank, today is February 16th, 2025.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=str,  \n",
    "    system_prompt=\"Use the customer's name while replying to them.\",  \n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt  \n",
    "def add_the_users_name(ctx: RunContext[str]) -> str:\n",
    "    return f\"The user's name is {ctx.deps}.\"\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "def add_the_date() -> str:  \n",
    "    return f'The date is {date.today()}.'\n",
    "\n",
    "\n",
    "result = agent.run_sync('What is the date?', deps='Frank')\n",
    "print(result.data)\n",
    "#> Hello Frank, the date today is 2032-01-02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "Who was Albert Einstein?\n",
      "response\n",
      "Albert Einstein (1879-1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).  His work is also known for its influence on the philosophy of science.\n",
      "\n",
      "He is best known for his mass–energy equivalence formula  E=mc², which has been dubbed \"the world's most famous equation\".  His contributions to physics include:\n",
      "\n",
      "* **The Special Theory of Relativity:** This theory revolutionized our understanding of space, time, gravity, and the universe at large, showing that space and time are relative and interconnected.\n",
      "* **The General Theory of Relativity:**  An extension of the special theory, this theory provided a new understanding of gravity as a curvature of spacetime caused by mass and energy.  It predicted phenomena like gravitational lensing and gravitational waves, which have since been observed.\n",
      "* **The Photoelectric Effect:** His explanation of this phenomenon, which describes how light can knock electrons off a metal surface, earned him the Nobel Prize in Physics in 1921.  This work was crucial for the development of quantum mechanics.\n",
      "\n",
      "Beyond his scientific achievements, Einstein was a humanist and pacifist who was deeply concerned about social justice and the potential dangers of unchecked technological advancement. He became a cultural icon, representing genius and intellectual curiosity.  He is considered one of the most influential scientists of all time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in result1._all_messages:\n",
    "    print (m.kind)\n",
    "    if m.kind == \"request\": \n",
    "        for part in m.parts:\n",
    "            print (part.content)\n",
    "    if m.kind == \"response\": \n",
    "        for part in m.parts:\n",
    "            print (part.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id=5 message='Hi John, would you like to grab a coffee next week?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent, RunContext, ModelRetry\n",
    "\n",
    "class DatabaseConn(BaseModel):\n",
    "    mood: str\n",
    "\n",
    "\n",
    "class ChatResult(BaseModel):\n",
    "    user_id: int\n",
    "    message: str\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=DatabaseConn,\n",
    "    result_type=ChatResult,\n",
    ")\n",
    "\n",
    "\n",
    "@agent.tool(retries=2)\n",
    "def get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n",
    "    \"\"\"Get a user's ID from their full name.\"\"\"\n",
    "    print(name)\n",
    "    #> John\n",
    "    #> John Doe\n",
    "    user_id = '5'#ctx.deps.mood\n",
    "    if user_id is None:\n",
    "        raise ModelRetry(\n",
    "            f'No user found with name {name!r}, remember to provide their full name'\n",
    "        )\n",
    "    return user_id\n",
    "\n",
    "\n",
    "result = agent.run_sync(\n",
    "    'Send a message to John Doe asking for coffee next week', deps=\"Sad\"\n",
    ")\n",
    "print(result.data)\n",
    "\"\"\"\n",
    "user_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided code does not calculate the volume.  The `calc_volume` function is empty.  I need a corrected `calc_volume` function to answer your question.  Please provide a function that calculates the volume given a size.  For example, if it's a cube, the volume is size³.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash')\n",
    "\n",
    "\n",
    "@agent.tool_plain\n",
    "def calc_volume(size: int) -> int:  \n",
    "    if size == 42:\n",
    "        return size**3\n",
    "    else:\n",
    "        raise ModelRetry('Please try again.')\n",
    "\n",
    "\n",
    "with capture_run_messages() as messages:  \n",
    "    try:\n",
    "        result = agent.run_sync('Please get me the volume of a box with size 6.')\n",
    "    except UnexpectedModelBehavior as e:\n",
    "        print('An error occurred:', e)\n",
    "        #> An error occurred: Tool exceeded max retries count of 1\n",
    "        print('cause:', repr(e.__cause__))\n",
    "        #> cause: ModelRetry('Please try again.')\n",
    "        print('messages:', messages)\n",
    "        \"\"\"\n",
    "        messages:\n",
    "        [\n",
    "            ModelRequest(\n",
    "                parts=[\n",
    "                    UserPromptPart(\n",
    "                        content='Please get me the volume of a box with size 6.',\n",
    "                        timestamp=datetime.datetime(...),\n",
    "                        part_kind='user-prompt',\n",
    "                    )\n",
    "                ],\n",
    "                kind='request',\n",
    "            ),\n",
    "            ModelResponse(\n",
    "                parts=[\n",
    "                    ToolCallPart(\n",
    "                        tool_name='calc_volume',\n",
    "                        args={'size': 6},\n",
    "                        tool_call_id=None,\n",
    "                        part_kind='tool-call',\n",
    "                    )\n",
    "                ],\n",
    "                model_name='function:model_logic',\n",
    "                timestamp=datetime.datetime(...),\n",
    "                kind='response',\n",
    "            ),\n",
    "            ModelRequest(\n",
    "                parts=[\n",
    "                    RetryPromptPart(\n",
    "                        content='Please try again.',\n",
    "                        tool_name='calc_volume',\n",
    "                        tool_call_id=None,\n",
    "                        timestamp=datetime.datetime(...),\n",
    "                        part_kind='retry-prompt',\n",
    "                    )\n",
    "                ],\n",
    "                kind='request',\n",
    "            ),\n",
    "            ModelResponse(\n",
    "                parts=[\n",
    "                    ToolCallPart(\n",
    "                        tool_name='calc_volume',\n",
    "                        args={'size': 6},\n",
    "                        tool_call_id=None,\n",
    "                        part_kind='tool-call',\n",
    "                    )\n",
    "                ],\n",
    "                model_name='function:model_logic',\n",
    "                timestamp=datetime.datetime(...),\n",
    "                kind='response',\n",
    "            ),\n",
    "        ]\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyDeps:  \n",
    "    api_key: str\n",
    "    http_client: httpx.AsyncClient\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=MyDeps,  \n",
    ")\n",
    "\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    deps = MyDeps('foobar', client)\n",
    "    result = await agent.run(\n",
    "        'Tell me a joke.',\n",
    "        deps=deps,  \n",
    "    )\n",
    "    print(result.data)\n",
    "    #> Did you hear about the toothpaste scandal? They called it Colgate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyDeps:\n",
    "    api_key: str\n",
    "    http_client: httpx.AsyncClient\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=MyDeps,\n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt  \n",
    "async def get_system_prompt(ctx: RunContext[MyDeps]) -> str:  \n",
    "    response = await ctx.deps.http_client.get(  \n",
    "        'https://example.com',\n",
    "        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},  \n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print (response.text)\n",
    "    return f'Prompt: {response.text}'\n",
    "\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    deps = MyDeps('foobar', client)\n",
    "    result = await agent.run('Tell me a joke.', deps=deps)\n",
    "    print(result.data)\n",
    "    #> Did you hear about the toothpaste scandal? They called it Colgate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '403 Forbidden' for url 'https://example.com?query=I+cannot+tell+you+a+joke+because+the+available+tools+lack+the+necessary+functionality.%0A#validate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mAsyncClient() \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m     52\u001b[0m     deps \u001b[38;5;241m=\u001b[39m MyDeps(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoobar\u001b[39m\u001b[38;5;124m'\u001b[39m, client)\n\u001b[0;32m---> 53\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTell me a joke.\u001b[39m\u001b[38;5;124m'\u001b[39m, deps\u001b[38;5;241m=\u001b[39mdeps)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#> Did you hear about the toothpaste scandal? They called it Colgate.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_ai/agent.py:340\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    332\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m _agent_graph\u001b[38;5;241m.\u001b[39mUserPromptNode[AgentDepsT](\n\u001b[1;32m    333\u001b[0m         user_prompt\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m    334\u001b[0m         system_prompts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompts,\n\u001b[1;32m    335\u001b[0m         system_prompt_functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompt_functions,\n\u001b[1;32m    336\u001b[0m         system_prompt_dynamic_functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompt_dynamic_functions,\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# Actually run\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     end_result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    341\u001b[0m         start_node,\n\u001b[1;32m    342\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    343\u001b[0m         deps\u001b[38;5;241m=\u001b[39mgraph_deps,\n\u001b[1;32m    344\u001b[0m         infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Build final run result\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# We don't do any advanced checking if the data is actually from a final result or not\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mRunResult(\n\u001b[1;32m    350\u001b[0m     state\u001b[38;5;241m.\u001b[39mmessage_history,\n\u001b[1;32m    351\u001b[0m     new_message_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     state\u001b[38;5;241m.\u001b[39musage,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_graph/graph.py:187\u001b[0m, in \u001b[0;36mGraph.run\u001b[0;34m(self, start_node, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    185\u001b[0m next_node \u001b[38;5;241m=\u001b[39m start_node\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(next_node, history, state\u001b[38;5;241m=\u001b[39mstate, deps\u001b[38;5;241m=\u001b[39mdeps, infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_node, End):\n\u001b[1;32m    189\u001b[0m         history\u001b[38;5;241m.\u001b[39mappend(EndStep(result\u001b[38;5;241m=\u001b[39mnext_node))\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_graph/graph.py:263\u001b[0m, in \u001b[0;36mGraph.next\u001b[0;34m(self, node, history, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m     start_ts \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mnow_utc()\n\u001b[1;32m    262\u001b[0m     start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 263\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mrun(ctx)\n\u001b[1;32m    264\u001b[0m     duration \u001b[38;5;241m=\u001b[39m perf_counter() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    266\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    267\u001b[0m     NodeStep(state\u001b[38;5;241m=\u001b[39mstate, node\u001b[38;5;241m=\u001b[39mnode, start_ts\u001b[38;5;241m=\u001b[39mstart_ts, duration\u001b[38;5;241m=\u001b[39mduration, snapshot_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot_state)\n\u001b[1;32m    268\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_ai/_agent_graph.py:299\u001b[0m, in \u001b[0;36mHandleResponseNode.run\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_tool_calls_response(ctx, tool_calls, handle_span)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m texts:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_text_response(ctx, texts, handle_span)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedModelBehavior(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived empty model response\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_ai/_agent_graph.py:355\u001b[0m, in \u001b[0;36mHandleResponseNode._handle_text_response\u001b[0;34m(self, ctx, texts, handle_span)\u001b[0m\n\u001b[1;32m    353\u001b[0m result_data_input \u001b[38;5;241m=\u001b[39m cast(NodeRunEndT, text)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _validate_result(result_data_input, ctx, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _result\u001b[38;5;241m.\u001b[39mToolRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mincrement_retries(ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmax_result_retries)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_ai/_agent_graph.py:685\u001b[0m, in \u001b[0;36m_validate_result\u001b[0;34m(result_data, ctx, tool_call)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mresult_validators:\n\u001b[1;32m    684\u001b[0m     run_context \u001b[38;5;241m=\u001b[39m _build_run_context(ctx)\n\u001b[0;32m--> 685\u001b[0m     result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mvalidate(result_data, tool_call, run_context)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_data\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pydantic_ai/_result.py:57\u001b[0m, in \u001b[0;36mResultValidator.validate\u001b[0;34m(self, result, tool_call, run_context)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_async:\n\u001b[1;32m     56\u001b[0m     function \u001b[38;5;241m=\u001b[39m cast(Callable[[Any], Awaitable[T]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction)\n\u001b[0;32m---> 57\u001b[0m     result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     function \u001b[38;5;241m=\u001b[39m cast(Callable[[Any], T], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction)\n",
      "Cell \u001b[0;32mIn[96], line 47\u001b[0m, in \u001b[0;36mvalidate_result\u001b[0;34m(ctx, final_response)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelRetry(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_response\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m error_type \u001b[38;5;241m=\u001b[39m error_types\u001b[38;5;241m.\u001b[39mget(status_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid status code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '403 Forbidden' for url 'https://example.com?query=I+cannot+tell+you+a+joke+because+the+available+tools+lack+the+necessary+functionality.%0A#validate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyDeps:\n",
    "    api_key: str\n",
    "    http_client: httpx.AsyncClient\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=MyDeps,\n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "async def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n",
    "    response = await ctx.deps.http_client.get('https://example.com')\n",
    "    response.raise_for_status()\n",
    "    return f'Prompt: {response.text}'\n",
    "\n",
    "\n",
    "@agent.tool  \n",
    "async def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\n",
    "    response = await ctx.deps.http_client.get(\n",
    "        'https://example.com#jokes',\n",
    "        params={'subject': subject},\n",
    "        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "@agent.result_validator  \n",
    "async def validate_result(ctx: RunContext[MyDeps], final_response: str) -> str:\n",
    "    response = await ctx.deps.http_client.post(\n",
    "        'https://example.com#validate',\n",
    "        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n",
    "        params={'query': final_response},\n",
    "    )\n",
    "    if response.status_code == 400:\n",
    "        raise ModelRetry(f'invalid response: {response.text}')\n",
    "    response.raise_for_status()\n",
    "    return final_response\n",
    "\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    deps = MyDeps('foobar', client)\n",
    "    result = await agent.run('Tell me a joke.', deps=deps)\n",
    "    print(result.data)\n",
    "    #> Did you hear about the toothpaste scandal? They called it Colgate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the die roll is 4. Your guess was correct!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from pydantic_ai import Agent, RunContext, Tool\n",
    "\n",
    "\n",
    "def roll_die() -> str:\n",
    "    \"\"\"Roll a six-sided die and return the result.\"\"\"\n",
    "    return str(random.randint(1, 6))\n",
    "\n",
    "\n",
    "def get_player_name(ctx: RunContext[str]) -> str:\n",
    "    \"\"\"Get the player's name.\"\"\"\n",
    "    return ctx.deps\n",
    "\n",
    "\n",
    "agent_a = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    deps_type=str,\n",
    "    tools=[roll_die, get_player_name],  \n",
    ")\n",
    "agent_b = Agent(\n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    deps_type=str,\n",
    "    tools=[  \n",
    "        Tool(roll_die, takes_ctx=False),\n",
    "        Tool(get_player_name, takes_ctx=True),\n",
    "    ],\n",
    ")\n",
    "dice_result = agent_b.run_sync('My guess is 4', deps='Anne')\n",
    "print(dice_result.data)\n",
    "#> Congratulations Anne, you guessed correctly! You're a winner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foobar'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.messages import ModelMessage, ModelResponse, TextPart\n",
    "from pydantic_ai.models.function import AgentInfo, FunctionModel\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash')\n",
    "\n",
    "\n",
    "@agent.tool_plain(docstring_format='google', require_parameter_descriptions=True)\n",
    "def foobar(a: int, b: str, c: dict[str, list[float]]) -> str:\n",
    "    \"\"\"Get me foobar.\n",
    "\n",
    "    Args:\n",
    "        a: apple pie\n",
    "        b: banana cake\n",
    "        c: carrot smoothie\n",
    "    \"\"\"\n",
    "    return f'{a} {b} {c}'\n",
    "\n",
    "\n",
    "def print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n",
    "    tool = info.function_tools[0]\n",
    "    print(tool.description)\n",
    "    #> Get me foobar.\n",
    "    print(tool.parameters_json_schema)\n",
    "    \"\"\"\n",
    "    {\n",
    "        'properties': {\n",
    "            'a': {'description': 'apple pie', 'title': 'A', 'type': 'integer'},\n",
    "            'b': {'description': 'banana cake', 'title': 'B', 'type': 'string'},\n",
    "            'c': {\n",
    "                'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'},\n",
    "                'description': 'carrot smoothie',\n",
    "                'title': 'C',\n",
    "                'type': 'object',\n",
    "            },\n",
    "        },\n",
    "        'required': ['a', 'b', 'c'],\n",
    "        'type': 'object',\n",
    "        'additionalProperties': False,\n",
    "    }\n",
    "    \"\"\"\n",
    "    return ModelResponse(parts=[TextPart('foobar')])\n",
    "\n",
    "\n",
    "result = agent.run_sync('hello', model=FunctionModel(print_schema))\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete! How can I assist you further?\n",
      "\n",
      "What do you want to test?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.tools import ToolDefinition\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash')\n",
    "\n",
    "\n",
    "async def only_if_42(\n",
    "    ctx: RunContext[int], tool_def: ToolDefinition\n",
    ") -> Union[ToolDefinition, None]:\n",
    "    if ctx.deps == 42:\n",
    "        return tool_def\n",
    "\n",
    "\n",
    "@agent.tool(prepare=only_if_42)\n",
    "def hitchhiker(ctx: RunContext[int], answer: str) -> str:\n",
    "    return f'{ctx.deps} {answer}'\n",
    "\n",
    "\n",
    "result = agent.run_sync('testing...', deps=41)\n",
    "print(result.data)\n",
    "#> success (no tool calls)\n",
    "result = agent.run_sync('testing...', deps=42)\n",
    "print(result.data)\n",
    "#> {\"hitchhiker\":\"42 a\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='London' country='UK'\n",
      "Usage(requests=1, request_tokens=27, response_tokens=7, total_tokens=34, details=None)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "class CityLocation(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash', result_type=CityLocation)\n",
    "result = agent.run_sync('Where were the olympics held in 2012?')\n",
    "print(result.data)\n",
    "#> city='London' country='United Kingdom'\n",
    "print(result.usage())\n",
    "\"\"\"\n",
    "Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width=30 height=20 depth=10 units='?'\n",
      "width=10 height=20 depth=30 units='cm'\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "class Box(BaseModel):\n",
    "    width: int\n",
    "    height: int\n",
    "    depth: int\n",
    "    units: str\n",
    "\n",
    "\n",
    "agent: Agent[None, Union[Box, str]] = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    result_type=Union[Box, str],  # type: ignore\n",
    "    system_prompt=(\n",
    "        \"Extract me the dimensions of a box, \"\n",
    "        \"if you can't extract all data, ask the user to try again.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = agent.run_sync('The box is 10x20x30')\n",
    "print(result.data)\n",
    "#> Please provide the units for the dimensions (e.g., cm, in, m).\n",
    "\n",
    "result = agent.run_sync('The box is 10x20x30 cm')\n",
    "print(result.data)\n",
    "#> width=10 height=20 depth=30 units='cm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'blue', 'green']\n",
      "[10, 20, 30]\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent: Agent[None, Union[list[str], list[int]]] = Agent(\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    result_type=Union[list[str], list[int]],  # type: ignore\n",
    "    system_prompt='Extract either colors or sizes from the shapes provided.',\n",
    ")\n",
    "\n",
    "result = agent.run_sync('red square, blue circle, green triangle')\n",
    "print(result.data)\n",
    "#> ['red', 'blue', 'green']\n",
    "\n",
    "result = agent.run_sync('square size 10, circle size 20, triangle size 30')\n",
    "print(result.data)\n",
    "#> [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " \"Hello, world!\" program originates from Brian Kernighan's 1\n",
      "972 tutorial, \"A Tutorial Introduction to the Language B\".  While\n",
      " not the very first use of the phrase in a programming context (some earlier examples exist, though they are less well-known), Kernighan's tutorial popularized it, and it became the canonical introductory program in nearly every programming language ever since.  Its simplicity and universality made it an ideal way to demonstrate the basic structure and syntax of a new language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash')  \n",
    "\n",
    "\n",
    "async with agent.run_stream('Where does \"hello world\" come from?') as result:  \n",
    "    async for message in result.stream_text(delta=True):  \n",
    "        print(message)\n",
    "        #> The first known\n",
    "        #> The first known use of \"hello,\n",
    "        #> The first known use of \"hello, world\" was in\n",
    "        #> The first known use of \"hello, world\" was in a 1974 textbook\n",
    "        #> The first known use of \"hello, world\" was in a 1974 textbook about the C\n",
    "        #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Ben'}\n",
      "{'name': 'Ben'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "class UserProfile(TypedDict, total=False):\n",
    "    name: str\n",
    "    dob: date\n",
    "    bio: str\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    result_type=UserProfile,\n",
    "    system_prompt='Extract a user profile from the input',\n",
    ")\n",
    "\n",
    "\n",
    "user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n",
    "async with agent.run_stream(user_input) as result:\n",
    "    async for profile in result.stream():\n",
    "        print(profile)\n",
    "        #> {'name': 'Ben'}\n",
    "        #> {'name': 'Ben'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Ben'}\n",
      "{'name': 'Ben'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "from pydantic import ValidationError\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "class UserProfile(TypedDict, total=False):\n",
    "    name: str\n",
    "    dob: date\n",
    "    bio: str\n",
    "\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash', result_type=UserProfile,system_prompt='Extract a user profile from the input')\n",
    "\n",
    "\n",
    "user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n",
    "async with agent.run_stream(user_input) as result:\n",
    "    async for message, last in result.stream_structured(debounce_by=0.01):  \n",
    "        try:\n",
    "            profile = await result.validate_structured_result(  \n",
    "                message,\n",
    "                allow_partial=not last,\n",
    "            )\n",
    "        except ValidationError:\n",
    "            continue\n",
    "        print(profile)\n",
    "        #> {'name': 'Ben'}\n",
    "        #> {'name': 'Ben'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
    "        #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "response\n"
     ]
    }
   ],
   "source": [
    "for m in result1.all_messages():\n",
    "    print (m.kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 2, 16, 18, 18, 6, 939030, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\", part_kind='text')], model_name='gemini-2.0-flash', timestamp=datetime.datetime(2025, 2, 16, 18, 18, 7, 546875, tzinfo=datetime.timezone.utc), kind='response')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash', system_prompt='Be a helpful assistant.')\n",
    "\n",
    "result = agent.run_sync('Tell me a joke.')\n",
    "print(result.data)\n",
    "#> Did you hear about the toothpaste scandal? They called it Colgate.\n",
    "\n",
    "# all messages from the run\n",
    "display(result.all_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 2, 16, 18, 21, 40, 898025, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')]\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 2, 16, 18, 21, 40, 898025, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\", part_kind='text')], model_name='gemini-2.0-flash', timestamp=datetime.datetime(2025, 2, 16, 18, 21, 41, 338188, tzinfo=datetime.timezone.utc), kind='response')]\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash', system_prompt='Be a helpful assistant.')\n",
    "\n",
    "\n",
    "async with agent.run_stream('Tell me a joke.') as result:\n",
    "    # incomplete messages before the stream finishes\n",
    "    print(result.all_messages())\n",
    "\n",
    "    async for text in result.stream_text():\n",
    "        print(text)\n",
    "\n",
    "    print(result.all_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for text in result.stream_text():\n",
    "    print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Why don't scientists trust atoms?                                                                                  \n",
       "\n",
       "Because they make up everything!                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Why don't scientists trust atoms?                                                                                  \n",
       "\n",
       "Because they make up everything!                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The joke plays on the double meaning of the phrase \"make up.\"                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Literal meaning:</span> Atoms are the fundamental building blocks of everything in the universe. They literally \"make  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>up\" all matter.                                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Figurative meaning:</span> \"Make up\" can also mean to invent or fabricate something, like a lie or an excuse.          \n",
       "\n",
       "So, the joke suggests that scientists don't trust atoms because they are unreliable and might be inventing things, \n",
       "just like someone who is \"making up\" a story. It's a pun!                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "The joke plays on the double meaning of the phrase \"make up.\"                                                      \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mLiteral meaning:\u001b[0m Atoms are the fundamental building blocks of everything in the universe. They literally \"make  \n",
       "\u001b[1;33m   \u001b[0mup\" all matter.                                                                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFigurative meaning:\u001b[0m \"Make up\" can also mean to invent or fabricate something, like a lie or an excuse.          \n",
       "\n",
       "So, the joke suggests that scientists don't trust atoms because they are unreliable and might be inventing things, \n",
       "just like someone who is \"making up\" a story. It's a pun!                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 56, 96466, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\", part_kind='text')], model_name='gemini-2.0-flash', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 56, 711543, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[UserPromptPart(content='Explain?', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 56, 718838, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content='The joke plays on the double meaning of the phrase \"make up.\"\\n\\n*   **Literal meaning:** Atoms are the fundamental building blocks of everything in the universe. They literally \"make up\" all matter.\\n*   **Figurative meaning:** \"Make up\" can also mean to invent or fabricate something, like a lie or an excuse.\\n\\nSo, the joke suggests that scientists don\\'t trust atoms because they are unreliable and might be inventing things, just like someone who is \"making up\" a story. It\\'s a pun!\\n', part_kind='text')], model_name='gemini-2.0-flash', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 58, 72299, tzinfo=datetime.timezone.utc), kind='response')]\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-2.0-flash', system_prompt='Be a helpful assistant.')\n",
    "\n",
    "result1 = agent.run_sync('Tell me a joke.')\n",
    "display(Markdown(result1.data))\n",
    "#> Did you hear about the toothpaste scandal? They called it Colgate.\n",
    "\n",
    "result2 = agent.run_sync('Explain?', message_history=result1.new_messages())\n",
    "display(Markdown(result2.data))\n",
    "#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n",
    "\n",
    "print(result2.all_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemPromptPart(content='Be a helpful assistant.', dynamic_ref=None, part_kind='system-prompt')\n",
      "Be a helpful assistant.\n",
      "UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 56, 96466, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n",
      "Tell me a joke.\n",
      "TextPart(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\", part_kind='text')\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "UserPromptPart(content='Explain?', timestamp=datetime.datetime(2025, 2, 16, 18, 24, 56, 718838, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n",
      "Explain?\n",
      "TextPart(content='The joke plays on the double meaning of the phrase \"make up.\"\\n\\n*   **Literal meaning:** Atoms are the fundamental building blocks of everything in the universe. They literally \"make up\" all matter.\\n*   **Figurative meaning:** \"Make up\" can also mean to invent or fabricate something, like a lie or an excuse.\\n\\nSo, the joke suggests that scientists don\\'t trust atoms because they are unreliable and might be inventing things, just like someone who is \"making up\" a story. It\\'s a pun!\\n', part_kind='text')\n",
      "The joke plays on the double meaning of the phrase \"make up.\"\n",
      "\n",
      "*   **Literal meaning:** Atoms are the fundamental building blocks of everything in the universe. They literally \"make up\" all matter.\n",
      "*   **Figurative meaning:** \"Make up\" can also mean to invent or fabricate something, like a lie or an excuse.\n",
      "\n",
      "So, the joke suggests that scientists don't trust atoms because they are unreliable and might be inventing things, just like someone who is \"making up\" a story. It's a pun!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in result2.all_messages():\n",
    "    for part in m.parts:\n",
    "        print (part)\n",
    "        print (part.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Why did the scarecrow win an award? Because he was outstanding in his field!\n",
      "\n",
      "Usage(requests=3, request_tokens=148, response_tokens=76, total_tokens=224, details=None)\n",
      "SystemPromptPart(content='Use the `joke_factory` to generate some jokes, then choose the best. You must return just a single joke.', dynamic_ref=None, part_kind='system-prompt')\n",
      "UserPromptPart(content='Tell me a jokes.', timestamp=datetime.datetime(2025, 2, 16, 19, 4, 48, 528701, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n",
      "ToolCallPart(tool_name='joke_factory', args={'count': 3}, tool_call_id=None, part_kind='tool-call')\n",
      "ToolReturnPart(tool_name='joke_factory', content=[\"Why don't scientists trust atoms? Because they make up everything!\", 'Parallel lines have so much in common. It’s a shame they’ll never meet.', 'Why did the scarecrow win an award? Because he was outstanding in his field!'], tool_call_id=None, timestamp=datetime.datetime(2025, 2, 16, 19, 4, 50, 43371, tzinfo=datetime.timezone.utc), part_kind='tool-return')\n",
      "TextPart(content='Why did the scarecrow win an award? Because he was outstanding in his field!\\n', part_kind='text')\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.usage import UsageLimits\n",
    "\n",
    "joke_selection_agent = Agent(  \n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    system_prompt=(\n",
    "        'Use the `joke_factory` to generate some jokes, then choose the best. '\n",
    "        'You must return just a single joke.'\n",
    "    ),\n",
    ")\n",
    "joke_generation_agent = Agent(  \n",
    "    'google-gla:gemini-2.0-flash', result_type=list[str]\n",
    ")\n",
    "\n",
    "\n",
    "@joke_selection_agent.tool\n",
    "async def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n",
    "    print (count)\n",
    "    r = await joke_generation_agent.run(  \n",
    "        f'Please generate {count} jokes.',\n",
    "        usage=ctx.usage,  \n",
    "    )\n",
    "    return r.data  \n",
    "\n",
    "\n",
    "result = joke_selection_agent.run_sync(\n",
    "    'Tell me a jokes.',\n",
    "    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),\n",
    ")\n",
    "print(result.data)\n",
    "#> Did you hear about the toothpaste scandal? They called it Colgate.\n",
    "print(result.usage())\n",
    "\"\"\"\n",
    "Usage(\n",
    "    requests=3, request_tokens=204, response_tokens=24, total_tokens=228, details=None\n",
    ")\n",
    "\"\"\"\n",
    "for msg in result.all_messages():\n",
    "    for part in msg.parts:\n",
    "        print (part)\n",
    "        #print (part.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemPromptPart(content='Use the `joke_factory` to generate some jokes, then choose the best. You must return just a single joke.', dynamic_ref=None, part_kind='system-prompt')\n",
      "UserPromptPart(content='Tell me two jokes.', timestamp=datetime.datetime(2025, 2, 16, 18, 55, 10, 354389, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n",
      "ToolCallPart(tool_name='joke_factory', args={'count': 2}, tool_call_id=None, part_kind='tool-call')\n",
      "ToolReturnPart(tool_name='joke_factory', content=[\"Why don't scientists trust atoms? Because they make up everything!\", 'Parallel lines have so much in common. It’s a shame they’ll never meet.'], tool_call_id=None, timestamp=datetime.datetime(2025, 2, 16, 18, 55, 11, 782871, tzinfo=datetime.timezone.utc), part_kind='tool-return')\n",
      "ToolCallPart(tool_name='final_result', args={'response': [\"Why don't scientists trust atoms? Because they make up everything!\"]}, tool_call_id=None, part_kind='tool-call')\n",
      "ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id=None, timestamp=datetime.datetime(2025, 2, 16, 18, 55, 12, 498057, tzinfo=datetime.timezone.utc), part_kind='tool-return')\n"
     ]
    }
   ],
   "source": [
    "for msg in result.all_messages():\n",
    "    for part in msg.parts:\n",
    "        print (part)\n",
    "        #print (part.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I need a topic for the joke. What subject would you like a joke about?\n",
      "\n",
      "Usage(requests=1, request_tokens=40, response_tokens=23, total_tokens=63, details=None)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClientAndKey:  \n",
    "    http_client: httpx.AsyncClient\n",
    "    api_key: str\n",
    "\n",
    "\n",
    "joke_selection_agent = Agent(\n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    deps_type=ClientAndKey,  \n",
    "    system_prompt=(\n",
    "        'Use the `joke_factory` tool to generate some jokes on the given subject, '\n",
    "        'then choose the best. You must return just a single joke.'\n",
    "    ),\n",
    ")\n",
    "joke_generation_agent = Agent(\n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    deps_type=ClientAndKey,  \n",
    "    result_type=list[str],\n",
    "    system_prompt=(\n",
    "        'Use the \"get_jokes\" tool to get some jokes on the given subject, '\n",
    "        'then extract each joke into a list.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@joke_selection_agent.tool\n",
    "async def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\n",
    "    r = await joke_generation_agent.run(\n",
    "        f'Please generate {count} jokes.',\n",
    "        deps=ctx.deps,  \n",
    "        usage=ctx.usage,\n",
    "    )\n",
    "    return r.data\n",
    "\n",
    "\n",
    "@joke_generation_agent.tool  \n",
    "async def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\n",
    "    response = await ctx.deps.http_client.get(\n",
    "        'https://example.com',\n",
    "        params={'count': count},\n",
    "        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    deps = ClientAndKey(client, 'foobar')\n",
    "    result = await joke_selection_agent.run('Tell me a joke.', deps=deps)\n",
    "    print(result.data)\n",
    "    #> Did you hear about the toothpaste scandal? They called it Colgate.\n",
    "    print(result.usage())  \n",
    "    \"\"\"\n",
    "    Usage(\n",
    "        requests=4,\n",
    "        request_tokens=309,\n",
    "        response_tokens=32,\n",
    "        total_tokens=341,\n",
    "        details=None,\n",
    "    )\n",
    "    \"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Union\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.prompt import Prompt\n",
    "\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from pydantic_ai.usage import Usage, UsageLimits\n",
    "\n",
    "\n",
    "class FlightDetails(BaseModel):\n",
    "    flight_number: str\n",
    "\n",
    "\n",
    "class Failed(BaseModel):\n",
    "    \"\"\"Unable to find a satisfactory choice.\"\"\"\n",
    "\n",
    "\n",
    "flight_search_agent = Agent[None, Union[FlightDetails, Failed]](  \n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    result_type=Union[FlightDetails, Failed],  # type: ignore\n",
    "    system_prompt=(\n",
    "        'Use the \"flight_search\" tool to find a flight '\n",
    "        'from the given origin to the given destination.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@flight_search_agent.tool  \n",
    "async def flight_search(\n",
    "    ctx: RunContext[None], origin: str, destination: str\n",
    ") -> Union[FlightDetails, None]:\n",
    "    # in reality, this would call a flight search API or\n",
    "    # use a browser to scrape a flight search website\n",
    "    return FlightDetails(flight_number='AK456')\n",
    "\n",
    "\n",
    "usage_limits = UsageLimits(request_limit=15)  \n",
    "\n",
    "\n",
    "async def find_flight(usage: Usage) -> Union[FlightDetails, None]:  \n",
    "    message_history: Union[list[ModelMessage], None] = None\n",
    "    for _ in range(3):\n",
    "        prompt = Prompt.ask(\n",
    "            'Where would you like to fly from and to?',\n",
    "        )\n",
    "        result = await flight_search_agent.run(\n",
    "            prompt,\n",
    "            message_history=message_history,\n",
    "            usage=usage,\n",
    "            usage_limits=usage_limits,\n",
    "        )\n",
    "        if isinstance(result.data, FlightDetails):\n",
    "            return result.data\n",
    "        else:\n",
    "            message_history = result.all_messages(\n",
    "                result_tool_return_content='Please try again.'\n",
    "            )\n",
    "\n",
    "\n",
    "class SeatPreference(BaseModel):\n",
    "    row: int = Field(ge=1, le=30)\n",
    "    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "\n",
    "# This agent is responsible for extracting the user's seat selection\n",
    "seat_preference_agent = Agent[None, Union[SeatPreference, Failed]](  \n",
    "    'google-gla:gemini-2.0-flash',\n",
    "    result_type=Union[SeatPreference, Failed],  # type: ignore\n",
    "    system_prompt=(\n",
    "        \"Extract the user's seat preference. \"\n",
    "        'Seats A and F are window seats. '\n",
    "        'Row 1 is the front row and has extra leg room. '\n",
    "        'Rows 14, and 20 also have extra leg room. '\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "async def find_seat(usage: Usage) -> SeatPreference:  \n",
    "    message_history: Union[list[ModelMessage], None] = None\n",
    "    while True:\n",
    "        answer = Prompt.ask('What seat would you like?')\n",
    "\n",
    "        result = await seat_preference_agent.run(\n",
    "            answer,\n",
    "            message_history=message_history,\n",
    "            usage=usage,\n",
    "            usage_limits=usage_limits,\n",
    "        )\n",
    "        if isinstance(result.data, SeatPreference):\n",
    "            return result.data\n",
    "        else:\n",
    "            print('Could not understand seat preference. Please try again.')\n",
    "            message_history = result.all_messages()\n",
    "\n",
    "\n",
    "usage: Usage = Usage()\n",
    "\n",
    "opt_flight_details = await find_flight(usage)\n",
    "if opt_flight_details is not None:\n",
    "    print(f'Flight found: {opt_flight_details.flight_number}')\n",
    "    #> Flight found: AK456\n",
    "    seat_preference = await find_seat(usage)\n",
    "    print(f'Seat preference: {seat_preference}')\n",
    "    #> Seat preference: row=1 seat='A'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
