{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import gradio as gr\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = '/Users/jsmidt/Documents/Github/GraduateAssistant/resources/092706_1_online.pdf'  # Replace with your PDF path\n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=2000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "text_chunks = split_text_into_chunks(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/3bt8jb_n4vvd559q56xkgl740008fx/T/ipykernel_45181/1267071680.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/var/folders/kg/3bt8jb_n4vvd559q56xkgl740008fx/T/ipykernel_45181/1267071680.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['163e49e4-2785-4613-bd8b-45b3016ebb20',\n",
       " '98bf1f63-3f2b-4f10-9d48-75e394d50cf5',\n",
       " 'ebe8e398-1a80-4bef-bac2-3c90bd0051e4',\n",
       " '9ebbb76e-6e3a-4dcf-925c-d986dde8c2c2',\n",
       " '8081e111-145c-47bf-ad3c-cbb7a403946d',\n",
       " '171afc11-941b-4f6a-9df3-10bcb4aad692',\n",
       " 'e84dfbf7-afc1-4381-b732-cefdf6587e1c',\n",
       " '9207aea5-8573-41fb-b804-6fe0bcdd2bf6',\n",
       " '4f51b7d9-b1f2-46b9-9d97-a9af8a2a87a2',\n",
       " '9d6a8ca7-af8a-443d-b3c6-35e7617843a2',\n",
       " 'e659009e-e1c6-432b-b061-5a89a7de7721',\n",
       " 'fe769438-1c1d-4b72-a26c-75abff19ae2a',\n",
       " 'b48e2df5-455a-41e8-a10a-0de1da988b52',\n",
       " 'f6e983da-95f8-4475-9c08-ac5f7096cc76',\n",
       " '38d1822e-6580-45d6-b790-822b6f07edaf',\n",
       " '43f4dba2-bea1-428f-b970-e6e7e36598f6',\n",
       " '20c6b75a-445c-4f31-b17e-09db0c053997',\n",
       " 'c6c8c92e-b592-4df4-836a-e88409240f39',\n",
       " 'e3f3d916-8f80-4ebf-95e9-000937c7cca0',\n",
       " '8a8e8129-eb37-479e-b0b8-4a237d6dc69c',\n",
       " '0bad8941-2427-4d3c-a0be-561052e50725',\n",
       " '4818ddf5-9a46-4fe2-895b-e7bbbb9c9bf2',\n",
       " 'f70faae0-8a49-4c9c-b660-4d4580af7f5a',\n",
       " 'faca4c10-fecd-4c42-8685-f49b444cc149',\n",
       " '9d6502f7-fe5d-4649-ad81-b42bbc36332a',\n",
       " '19724cea-a02e-41ca-821d-28bda11cafe9',\n",
       " '54d63261-6c6f-444b-82d3-45b05762bcf2',\n",
       " 'e23a604c-528d-4471-aee3-47ae4732add7',\n",
       " '695a714f-3bbb-46d8-ac08-06cebe24e662',\n",
       " '22933656-3426-43c9-9061-1ef6a370892e',\n",
       " '955d6510-911f-4e79-b7b9-bc1197095efc',\n",
       " 'd8d0c1fb-afcd-4c75-91c0-289d6e0c7524',\n",
       " '53008c3b-4645-4e3d-844a-348d28f00231',\n",
       " '3d260d2d-417a-4fd7-9f15-0f12546a1af2',\n",
       " '04d0dc60-a88a-4abf-bd30-f633d0e4b130',\n",
       " '09fba2c4-3568-4d7a-a823-bbf55d8db54a',\n",
       " 'b0f1638a-8434-45d5-a36a-77a7be928f1d',\n",
       " '7b093aa5-2772-461f-b9f2-778351d9a671',\n",
       " '36dbc933-604f-4c74-abba-6d2bc28b78f5',\n",
       " '8f48c8ce-fcf4-4012-8834-71f6376e2823',\n",
       " 'da0effeb-417e-46d8-9f8d-d2928e991b53',\n",
       " '290852d1-5451-4be0-9452-2f7725401f8d',\n",
       " '1380bf9e-8f65-4e67-9d16-b192499db9d3',\n",
       " 'd1b064ca-806c-4d99-a158-bf868eb4ddea',\n",
       " '9ea713eb-0bfd-4e59-ac4c-dd6908f1fde3',\n",
       " '590963ab-89f0-4af5-8261-e6fedf40e7b0',\n",
       " '862adae4-6f1c-467f-83a9-b64af7a36419',\n",
       " 'd8803489-3d40-4e4b-bbb9-29ca3d500feb',\n",
       " '8df7503a-1460-4acb-a1ee-d1335ba916cb',\n",
       " '5ac103c1-3632-4fc7-a295-8355fab59abb',\n",
       " '6fcc0382-a289-4984-ae26-55ac983fccae',\n",
       " '72fc2407-aca3-4b5b-8162-0217d2e8b6ce',\n",
       " '99fdd06b-0d79-48f8-96a2-7b5e1b86e287',\n",
       " '82b13388-6833-403f-803d-62eeab441239',\n",
       " '47ed90b4-6606-488c-acb4-f967556aea25',\n",
       " '3350f64d-2334-4e73-b46b-daba2da57dc9',\n",
       " 'f7bf6f3e-18ae-4443-bc54-a7466028f45a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"disease_info\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "vector_store.add_texts(texts=text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(topk=3),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def get_response(user_query):\n",
    "    response = retrieval_chain({\n",
    "        \"question\": user_query,\n",
    "        \"chat_history\": conversation_history\n",
    "    })\n",
    "    conversation_history.append((user_query, response['answer']))\n",
    "    return response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsmidt/Library/Python/3.12/lib/python/site-packages/gradio/components/chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def chat_interface(user_input, history):\n",
    "    response = get_response(user_input)\n",
    "    history.append((user_input, response))\n",
    "    return history, history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State([])\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(show_label=False, placeholder=\"Enter your question...\")\n",
    "        submit_btn = gr.Button(\"Send\")\n",
    "    submit_btn.click(chat_interface, inputs=[user_input, state], outputs=[chatbot, state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMemory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquestion_generator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLMChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'answer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrephrase_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_generated_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mget_chat_history\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mresponse_if_no_docs_found\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrievers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      ".. deprecated:: 0.1.17 Use :meth:`~create_history_aware_retriever together with create_retrieval_chain (see example in docstring)` instead. It will not be removed until langchain==1.0.\n",
      "\n",
      "Chain for having a conversation based on retrieved documents.\n",
      "\n",
      "    This class is deprecated. See below for an example implementation using\n",
      "    `create_retrieval_chain`. Additional walkthroughs can be found at\n",
      "    https://python.langchain.com/docs/use_cases/question_answering/chat_history\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import (\n",
      "                create_history_aware_retriever,\n",
      "                create_retrieval_chain,\n",
      "            )\n",
      "            from langchain.chains.combine_documents import create_stuff_documents_chain\n",
      "            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "            from langchain_openai import ChatOpenAI\n",
      "\n",
      "\n",
      "            retriever = ...  # Your retriever\n",
      "\n",
      "            llm = ChatOpenAI()\n",
      "\n",
      "            # Contextualize question\n",
      "            contextualize_q_system_prompt = (\n",
      "                \"Given a chat history and the latest user question \"\n",
      "                \"which might reference context in the chat history, \"\n",
      "                \"formulate a standalone question which can be understood \"\n",
      "                \"without the chat history. Do NOT answer the question, just \"\n",
      "                \"reformulate it if needed and otherwise return it as is.\"\n",
      "            )\n",
      "            contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
      "                [\n",
      "                    (\"system\", contextualize_q_system_prompt),\n",
      "                    MessagesPlaceholder(\"chat_history\"),\n",
      "                    (\"human\", \"{input}\"),\n",
      "                ]\n",
      "            )\n",
      "            history_aware_retriever = create_history_aware_retriever(\n",
      "                llm, retriever, contextualize_q_prompt\n",
      "            )\n",
      "\n",
      "            # Answer question\n",
      "            qa_system_prompt = (\n",
      "                \"You are an assistant for question-answering tasks. Use \"\n",
      "                \"the following pieces of retrieved context to answer the \"\n",
      "                \"question. If you don't know the answer, just say that you \"\n",
      "                \"don't know. Use three sentences maximum and keep the answer \"\n",
      "                \"concise.\"\n",
      "                \"\n",
      "\n",
      "\"\n",
      "                \"{context}\"\n",
      "            )\n",
      "            qa_prompt = ChatPromptTemplate.from_messages(\n",
      "                [\n",
      "                    (\"system\", qa_system_prompt),\n",
      "                    MessagesPlaceholder(\"chat_history\"),\n",
      "                    (\"human\", \"{input}\"),\n",
      "                ]\n",
      "            )\n",
      "            # Below we use create_stuff_documents_chain to feed all retrieved context\n",
      "            # into the LLM. Note that we can also use StuffDocumentsChain and other\n",
      "            # instances of BaseCombineDocumentsChain.\n",
      "            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
      "            rag_chain = create_retrieval_chain(\n",
      "                history_aware_retriever, question_answer_chain\n",
      "            )\n",
      "\n",
      "            # Usage:\n",
      "            chat_history = []  # Collect chat history here (a sequence of messages)\n",
      "            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
      "\n",
      "    This chain takes in chat history (a list of messages) and new questions,\n",
      "    and then returns an answer to that question.\n",
      "    The algorithm for this chain consists of three parts:\n",
      "\n",
      "    1. Use the chat history and the new question to create a \"standalone question\".\n",
      "    This is done so that this question can be passed into the retrieval step to fetch\n",
      "    relevant documents. If only the new question was passed in, then relevant context\n",
      "    may be lacking. If the whole conversation was passed into retrieval, there may\n",
      "    be unnecessary information there that would distract from retrieval.\n",
      "\n",
      "    2. This new question is passed to the retriever and relevant documents are\n",
      "    returned.\n",
      "\n",
      "    3. The retrieved documents are passed to an LLM along with either the new question\n",
      "    (default behavior) or the original question and chat history to generate a final\n",
      "    response.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import (\n",
      "                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
      "            )\n",
      "            from langchain_core.prompts import PromptTemplate\n",
      "            from langchain_community.llms import OpenAI\n",
      "\n",
      "            combine_docs_chain = StuffDocumentsChain(...)\n",
      "            vectorstore = ...\n",
      "            retriever = vectorstore.as_retriever()\n",
      "\n",
      "            # This controls how the standalone question is generated.\n",
      "            # Should take `chat_history` and `question` as input variables.\n",
      "            template = (\n",
      "                \"Combine the chat history and follow up question into \"\n",
      "                \"a standalone question. Chat History: {chat_history}\"\n",
      "                \"Follow up question: {question}\"\n",
      "            )\n",
      "            prompt = PromptTemplate.from_template(template)\n",
      "            llm = OpenAI()\n",
      "            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "            chain = ConversationalRetrievalChain(\n",
      "                combine_docs_chain=combine_docs_chain,\n",
      "                retriever=retriever,\n",
      "                question_generator=question_generator_chain,\n",
      "            )\n",
      "    \n",
      "\u001b[0;31mFile:\u001b[0m           ~/Library/Python/3.12/lib/python/site-packages/langchain/chains/conversational_retrieval/base.py\n",
      "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/3bt8jb_n4vvd559q56xkgl740008fx/T/ipykernel_45181/3916198089.py:4: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = retrieval_chain({\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ConversationalRetrievalChain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
