{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command\n",
    "\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'write_essay': 'An essay about topic: cat'}\n",
      "{'__interrupt__': (Interrupt(value={'essay': 'An essay about topic: cat', 'action': 'Please approve/reject the essay'}, resumable=True, ns=['workflow:0855fb8b-7fe5-eb8e-f570-2edc1bb0d804'], when='during'),)}\n",
      "{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': True}}\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def write_essay(topic: str) -> str:\n",
    "    \"\"\"Write an essay about the given topic.\"\"\"\n",
    "    time.sleep(1) # This is a placeholder for a long-running task.\n",
    "    return f\"An essay about topic: {topic}\"\n",
    "\n",
    "@entrypoint(checkpointer=MemorySaver())\n",
    "def workflow(topic: str) -> dict:\n",
    "    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n",
    "    essay = write_essay(\"cat\").result()\n",
    "    is_approved = interrupt({\n",
    "        \"essay\": essay, \n",
    "        \"action\": \"Please approve/reject the essay\",\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"essay\": essay, # The essay that was generated\n",
    "        \"is_approved\": is_approved, # Response from HIL\n",
    "    }\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n",
    "\n",
    "for item in workflow.stream(\"cat\", config):\n",
    "    print(item)\n",
    "\n",
    "# Get review from a user (e.g., via a UI)\n",
    "# In this case, we're using a bool, but this can be any json-serializable value.\n",
    "human_review = True\n",
    "\n",
    "hh = Command(resume=human_review)\n",
    "for item in workflow.stream(Command(resume=human_review), config):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@entrypoint(checkpointer=MemorySaver())\n",
    "def my_workflow(number: int, *, previous: Any = None) -> int:\n",
    "    previous = previous or 0\n",
    "    return number + previous\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"some_thread_id\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print (my_workflow.invoke(1, config))  # 1 (previous was None)\n",
    "my_workflow.invoke(2, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context\n",
    ")\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "joke_flow = Workflow(timeout=60, verbose=True)\n",
    "\n",
    "\n",
    "@step(workflow=joke_flow)\n",
    "async def generate_joke(ev: StartEvent) -> JokeEvent:\n",
    "    topic = ev.topic\n",
    "\n",
    "    prompt = f\"Write your best joke about {topic}.\"\n",
    "\n",
    "    response = await llm.acomplete(prompt)\n",
    "    print ('\\n * one * \\n')\n",
    "    print (response)\n",
    "    return JokeEvent(joke=str(response))\n",
    "\n",
    "\n",
    "@step(workflow=joke_flow)\n",
    "async def critique_joke(ev: JokeEvent) -> StopEvent:\n",
    "    joke = ev.joke\n",
    "\n",
    "    prompt = (\n",
    "        f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "    )\n",
    "    response = await llm.acomplete(prompt)\n",
    "\n",
    "    return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n",
      "<llama_index.core.workflow.context.Context object at 0x11a041d00>\n",
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "w = joke_flow\n",
    "\n",
    "handler = w.run(topic=\"Pirates\")\n",
    "result = await handler\n",
    "#print ('\\n * two * \\n')\n",
    "#print (result)\n",
    "\n",
    "print (handler.ctx)\n",
    "# continue with next run\n",
    "handler = w.run(ctx=handler.ctx, topic=\"Pirates\")\n",
    "result = await handler\n",
    "#print ('\\n * thee * \\n')\n",
    "#print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n",
      "here\n",
      "yo\n",
      "id_='f3cdb4ad-4c38-4262-b291-2c85af5075c1' last_completed_step='generate_joke' input_event=StartEvent() output_event=JokeEvent(joke='Why do pirates make terrible dancers?  Because they always have one leg!\\n') ctx_state={'globals': {}, 'streaming_queue': '[]', 'queues': {'aa5644d7-5345-46c5-bf3e-b320828cbe8e': '[\"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"_data\\\\\": {\\\\\"topic\\\\\": \\\\\"Pirates\\\\\"}}, \\\\\"qualified_name\\\\\": \\\\\"llama_index.core.workflow.events.StartEvent\\\\\"}\"]', '_done': '[]', 'generate_joke': '[]', 'critique_joke': '[]'}, 'stepwise': False, 'events_buffer': {}, 'in_progress': {'generate_joke': []}, 'accepted_events': [('generate_joke', 'StartEvent'), ('critique_joke', 'JokeEvent')], 'broker_log': ['{\"__is_pydantic\": true, \"value\": {\"_data\": {\"topic\": \"Pirates\"}}, \"qualified_name\": \"llama_index.core.workflow.events.StartEvent\"}'], 'waiter_id': 'aa5644d7-5345-46c5-bf3e-b320828cbe8e', 'is_running': True}\n",
      "yo\n",
      "id_='4a2df7c1-980e-4bab-9214-aaf05e55b21b' last_completed_step='critique_joke' input_event=JokeEvent(joke='Why do pirates make terrible dancers?  Because they always have one leg!\\n') output_event=StopEvent() ctx_state={'globals': {}, 'streaming_queue': '[]', 'queues': {'aa5644d7-5345-46c5-bf3e-b320828cbe8e': '[\"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"_data\\\\\": {\\\\\"topic\\\\\": \\\\\"Pirates\\\\\"}}, \\\\\"qualified_name\\\\\": \\\\\"llama_index.core.workflow.events.StartEvent\\\\\"}\", \"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"joke\\\\\": \\\\\"Why do pirates make terrible dancers?  Because they always have one leg!\\\\\\\\n\\\\\"}, \\\\\"qualified_name\\\\\": \\\\\"__main__.JokeEvent\\\\\"}\"]', '_done': '[]', 'generate_joke': '[]', 'critique_joke': '[]'}, 'stepwise': False, 'events_buffer': {}, 'in_progress': {'generate_joke': [], 'critique_joke': []}, 'accepted_events': [('generate_joke', 'StartEvent'), ('critique_joke', 'JokeEvent')], 'broker_log': ['{\"__is_pydantic\": true, \"value\": {\"_data\": {\"topic\": \"Pirates\"}}, \"qualified_name\": \"llama_index.core.workflow.events.StartEvent\"}', '{\"__is_pydantic\": true, \"value\": {\"joke\": \"Why do pirates make terrible dancers?  Because they always have one leg!\\\\n\"}, \"qualified_name\": \"__main__.JokeEvent\"}'], 'waiter_id': 'aa5644d7-5345-46c5-bf3e-b320828cbe8e', 'is_running': True}\n",
      "Running step critique_joke\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This joke relies on a simple pun, exploiting the double meaning of \"one leg.\"  Let\\'s analyze its strengths and weaknesses:\\n\\n**Strengths:**\\n\\n* **Simplicity and Immediate Understanding:** The setup is clear and concise.  Most people understand the stereotypical image of a pirate with a missing leg. The punchline is instantly grasped.  This makes it accessible to a wide audience.\\n* **Surprise and Unexpectedness:** While the setup hints at a physical limitation, the punchline plays on the literal interpretation of \"one leg\" in relation to dancing, creating a small but satisfying surprise.  The unexpectedness is key to the humor.\\n* **Reliance on a Common Stereotype:** The pirate with a missing leg is a well-established trope in popular culture. This pre-existing knowledge makes the joke\\'s premise instantly relatable and understandable, requiring minimal explanation.\\n\\n\\n**Weaknesses:**\\n\\n* **Over-reliance on a Single Pun:** The entire joke hinges on a single pun.  This limits its comedic potential and can feel somewhat simplistic or even childish to more sophisticated audiences.  There\\'s no layered humor or secondary meaning to explore.\\n* **Lack of Nuance or Wordplay Beyond the Pun:**  The joke doesn\\'t utilize any other forms of wordplay or clever phrasing. It\\'s a straightforward statement of the pun and nothing more.\\n* **Potential for Being Considered \"Dad Joke\" Level:**  The simplicity and reliance on a basic pun often categorize jokes like this as \"dad jokes\"â€”a genre often associated with predictable and slightly corny humor.  This isn\\'t inherently negative, but it does limit its appeal to certain audiences.\\n* **Dependence on a Stereotype:** While the pirate stereotype is widely known, relying solely on it can be seen as unoriginal or even insensitive if the stereotype is considered offensive or outdated.\\n\\n\\n**Overall Critique:**\\n\\nThe joke is effective in its simplicity and immediate understanding. It\\'s a good example of a low-effort, low-risk joke that\\'s likely to elicit a chuckle from a broad audience. However, its reliance on a single, simple pun and a potentially problematic stereotype limits its comedic sophistication and longevity.  It\\'s a perfectly acceptable joke for casual conversation or a lighthearted moment, but it wouldn\\'t be considered high-quality humor in a more demanding comedic context.  Its success depends entirely on the audience\\'s tolerance for simple puns and their familiarity with the pirate stereotype.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import WorkflowCheckpointer\n",
    "\n",
    "w = joke_flow\n",
    "w_cptr = WorkflowCheckpointer(workflow=w)\n",
    "\n",
    "# to checkpoint a run, use the `run` method from w_cptr\n",
    "handler = w_cptr.run(topic=\"Pirates\")\n",
    "await handler\n",
    "\n",
    "print ('here')\n",
    "# to view the stored checkpoints of this run\n",
    "for c in w_cptr.checkpoints[handler.run_id]:\n",
    "    print ('yo')\n",
    "    print (c)\n",
    "\n",
    "# to run from one of the checkpoints, use `run_from` method\n",
    "ckpt = w_cptr.checkpoints[handler.run_id][0]\n",
    "handler = w_cptr.run_from(topic=\"Ships\", checkpoint=ckpt)\n",
    "await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEvent(Event):\n",
    "    input: str\n",
    "\n",
    "\n",
    "class SetupEvent(Event):\n",
    "    error: bool\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CollectExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> SetupEvent:\n",
    "        # generically start everything up\n",
    "        if not hasattr(self, \"setup\") or not self.setup:\n",
    "            self.setup = True\n",
    "            print(\"I got set up\")\n",
    "        return SetupEvent(error=False)\n",
    "\n",
    "    @step\n",
    "    async def collect_input(self, ev: StartEvent) -> InputEvent:\n",
    "        if hasattr(ev, \"input\"):\n",
    "            # perhaps validate the input\n",
    "            print(\"I got some input\")\n",
    "            return InputEvent(input=ev.input)\n",
    "\n",
    "    @step\n",
    "    async def parse_query(self, ev: StartEvent) -> QueryEvent:\n",
    "        if hasattr(ev, \"query\"):\n",
    "            # parse the query in some way\n",
    "            print(\"I got a query\")\n",
    "            return QueryEvent(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def run_query(\n",
    "        self, ctx: Context, ev: InputEvent | SetupEvent | QueryEvent\n",
    "    ) -> StopEvent | None:\n",
    "        ready = ctx.collect_events(ev, [QueryEvent, InputEvent, SetupEvent])\n",
    "        if ready is None:\n",
    "            print(\"Not enough events yet\")\n",
    "            return None\n",
    "\n",
    "        # run the query\n",
    "        print(\"Now I have all the events\")\n",
    "        print(ready)\n",
    "\n",
    "        result = f\"Ran query '{ready[0].query}' on input '{ready[1].input}'\"\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got some input\n",
      "I got a query\n",
      "Not enough events yet\n",
      "Not enough events yet\n",
      "Now I have all the events\n",
      "[QueryEvent(query=\"Here's my question\"), InputEvent(input=\"Here's some input\"), SetupEvent(error=False)]\n",
      "Ran query 'Here's my question' on input 'Here's some input'\n"
     ]
    }
   ],
   "source": [
    "c = CollectExampleFlow()\n",
    "result = await c.run(input=\"Here's some input\", query=\"Here's my question\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent,\n",
    ")\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = FunctionAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant.\",\n",
    "    tools=[\n",
    "        FunctionTool.from_defaults(fn=add),\n",
    "        FunctionTool.from_defaults(fn=subtract),\n",
    "    ],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "retriever_agent = FunctionAgent(\n",
    "    name=\"retriever\",\n",
    "    description=\"Manages data retrieval\",\n",
    "    system_prompt=\"You are a retrieval assistant.\",\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[calculator_agent, retriever_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "\n",
    "#  Or stream the events\n",
    "handler = workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "async for event in handler.stream_events():\n",
    "    if hasattr(event, \"delta\"):\n",
    "        print(event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mReActOutputParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      ReAct Output parser.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/Github/100-days-of-grind/.venv/lib/python3.12/site-packages/llama_index/core/agent/react/output_parser.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "ReActOutputParser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
