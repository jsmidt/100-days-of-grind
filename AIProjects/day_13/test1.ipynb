{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command\n",
    "\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'write_essay': 'An essay about topic: cat'}\n",
      "{'__interrupt__': (Interrupt(value={'essay': 'An essay about topic: cat', 'action': 'Please approve/reject the essay'}, resumable=True, ns=['workflow:0855fb8b-7fe5-eb8e-f570-2edc1bb0d804'], when='during'),)}\n",
      "{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': True}}\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def write_essay(topic: str) -> str:\n",
    "    \"\"\"Write an essay about the given topic.\"\"\"\n",
    "    time.sleep(1) # This is a placeholder for a long-running task.\n",
    "    return f\"An essay about topic: {topic}\"\n",
    "\n",
    "@entrypoint(checkpointer=MemorySaver())\n",
    "def workflow(topic: str) -> dict:\n",
    "    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n",
    "    essay = write_essay(\"cat\").result()\n",
    "    is_approved = interrupt({\n",
    "        \"essay\": essay, \n",
    "        \"action\": \"Please approve/reject the essay\",\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"essay\": essay, # The essay that was generated\n",
    "        \"is_approved\": is_approved, # Response from HIL\n",
    "    }\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n",
    "\n",
    "for item in workflow.stream(\"cat\", config):\n",
    "    print(item)\n",
    "\n",
    "# Get review from a user (e.g., via a UI)\n",
    "# In this case, we're using a bool, but this can be any json-serializable value.\n",
    "human_review = True\n",
    "\n",
    "hh = Command(resume=human_review)\n",
    "for item in workflow.stream(Command(resume=human_review), config):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@entrypoint(checkpointer=MemorySaver())\n",
    "def my_workflow(number: int, *, previous: Any = None) -> int:\n",
    "    previous = previous or 0\n",
    "    return number + previous\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"some_thread_id\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print (my_workflow.invoke(1, config))  # 1 (previous was None)\n",
    "my_workflow.invoke(2, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context\n",
    ")\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "joke_flow = Workflow(timeout=60, verbose=True)\n",
    "\n",
    "\n",
    "@step(workflow=joke_flow)\n",
    "async def generate_joke(ev: StartEvent) -> JokeEvent:\n",
    "    topic = ev.topic\n",
    "\n",
    "    prompt = f\"Write your best joke about {topic}.\"\n",
    "\n",
    "    response = await llm.acomplete(prompt)\n",
    "    print ('\\n * one * \\n')\n",
    "    print (response)\n",
    "    return JokeEvent(joke=str(response))\n",
    "\n",
    "\n",
    "@step(workflow=joke_flow)\n",
    "async def critique_joke(ev: JokeEvent) -> StopEvent:\n",
    "    joke = ev.joke\n",
    "\n",
    "    prompt = (\n",
    "        f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "    )\n",
    "    response = await llm.acomplete(prompt)\n",
    "\n",
    "    return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n",
      "<llama_index.core.workflow.context.Context object at 0x11a041d00>\n",
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "w = joke_flow\n",
    "\n",
    "handler = w.run(topic=\"Pirates\")\n",
    "result = await handler\n",
    "#print ('\\n * two * \\n')\n",
    "#print (result)\n",
    "\n",
    "print (handler.ctx)\n",
    "# continue with next run\n",
    "handler = w.run(ctx=handler.ctx, topic=\"Pirates\")\n",
    "result = await handler\n",
    "#print ('\\n * thee * \\n')\n",
    "#print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step generate_joke\n",
      "\n",
      " * one * \n",
      "\n",
      "Why do pirates make terrible dancers?  Because they always have one leg!\n",
      "\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n",
      "here\n",
      "yo\n",
      "id_='f3cdb4ad-4c38-4262-b291-2c85af5075c1' last_completed_step='generate_joke' input_event=StartEvent() output_event=JokeEvent(joke='Why do pirates make terrible dancers?  Because they always have one leg!\\n') ctx_state={'globals': {}, 'streaming_queue': '[]', 'queues': {'aa5644d7-5345-46c5-bf3e-b320828cbe8e': '[\"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"_data\\\\\": {\\\\\"topic\\\\\": \\\\\"Pirates\\\\\"}}, \\\\\"qualified_name\\\\\": \\\\\"llama_index.core.workflow.events.StartEvent\\\\\"}\"]', '_done': '[]', 'generate_joke': '[]', 'critique_joke': '[]'}, 'stepwise': False, 'events_buffer': {}, 'in_progress': {'generate_joke': []}, 'accepted_events': [('generate_joke', 'StartEvent'), ('critique_joke', 'JokeEvent')], 'broker_log': ['{\"__is_pydantic\": true, \"value\": {\"_data\": {\"topic\": \"Pirates\"}}, \"qualified_name\": \"llama_index.core.workflow.events.StartEvent\"}'], 'waiter_id': 'aa5644d7-5345-46c5-bf3e-b320828cbe8e', 'is_running': True}\n",
      "yo\n",
      "id_='4a2df7c1-980e-4bab-9214-aaf05e55b21b' last_completed_step='critique_joke' input_event=JokeEvent(joke='Why do pirates make terrible dancers?  Because they always have one leg!\\n') output_event=StopEvent() ctx_state={'globals': {}, 'streaming_queue': '[]', 'queues': {'aa5644d7-5345-46c5-bf3e-b320828cbe8e': '[\"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"_data\\\\\": {\\\\\"topic\\\\\": \\\\\"Pirates\\\\\"}}, \\\\\"qualified_name\\\\\": \\\\\"llama_index.core.workflow.events.StartEvent\\\\\"}\", \"{\\\\\"__is_pydantic\\\\\": true, \\\\\"value\\\\\": {\\\\\"joke\\\\\": \\\\\"Why do pirates make terrible dancers?  Because they always have one leg!\\\\\\\\n\\\\\"}, \\\\\"qualified_name\\\\\": \\\\\"__main__.JokeEvent\\\\\"}\"]', '_done': '[]', 'generate_joke': '[]', 'critique_joke': '[]'}, 'stepwise': False, 'events_buffer': {}, 'in_progress': {'generate_joke': [], 'critique_joke': []}, 'accepted_events': [('generate_joke', 'StartEvent'), ('critique_joke', 'JokeEvent')], 'broker_log': ['{\"__is_pydantic\": true, \"value\": {\"_data\": {\"topic\": \"Pirates\"}}, \"qualified_name\": \"llama_index.core.workflow.events.StartEvent\"}', '{\"__is_pydantic\": true, \"value\": {\"joke\": \"Why do pirates make terrible dancers?  Because they always have one leg!\\\\n\"}, \"qualified_name\": \"__main__.JokeEvent\"}'], 'waiter_id': 'aa5644d7-5345-46c5-bf3e-b320828cbe8e', 'is_running': True}\n",
      "Running step critique_joke\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This joke relies on a simple pun, exploiting the double meaning of \"one leg.\"  Let\\'s analyze its strengths and weaknesses:\\n\\n**Strengths:**\\n\\n* **Simplicity and Immediate Understanding:** The setup is clear and concise.  Most people understand the stereotypical image of a pirate with a missing leg. The punchline is instantly grasped.  This makes it accessible to a wide audience.\\n* **Surprise and Unexpectedness:** While the setup hints at a physical limitation, the punchline plays on the literal interpretation of \"one leg\" in relation to dancing, creating a small but satisfying surprise.  The unexpectedness is key to the humor.\\n* **Reliance on a Common Stereotype:** The pirate with a missing leg is a well-established trope in popular culture. This pre-existing knowledge makes the joke\\'s premise instantly relatable and understandable, requiring minimal explanation.\\n\\n\\n**Weaknesses:**\\n\\n* **Over-reliance on a Single Pun:** The entire joke hinges on a single pun.  This limits its comedic potential and can feel somewhat simplistic or even childish to more sophisticated audiences.  There\\'s no layered humor or secondary meaning to explore.\\n* **Lack of Nuance or Wordplay Beyond the Pun:**  The joke doesn\\'t utilize any other forms of wordplay or clever phrasing. It\\'s a straightforward statement of the pun and nothing more.\\n* **Potential for Being Considered \"Dad Joke\" Level:**  The simplicity and reliance on a basic pun often categorize jokes like this as \"dad jokes\"—a genre often associated with predictable and slightly corny humor.  This isn\\'t inherently negative, but it does limit its appeal to certain audiences.\\n* **Dependence on a Stereotype:** While the pirate stereotype is widely known, relying solely on it can be seen as unoriginal or even insensitive if the stereotype is considered offensive or outdated.\\n\\n\\n**Overall Critique:**\\n\\nThe joke is effective in its simplicity and immediate understanding. It\\'s a good example of a low-effort, low-risk joke that\\'s likely to elicit a chuckle from a broad audience. However, its reliance on a single, simple pun and a potentially problematic stereotype limits its comedic sophistication and longevity.  It\\'s a perfectly acceptable joke for casual conversation or a lighthearted moment, but it wouldn\\'t be considered high-quality humor in a more demanding comedic context.  Its success depends entirely on the audience\\'s tolerance for simple puns and their familiarity with the pirate stereotype.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import WorkflowCheckpointer\n",
    "\n",
    "w = joke_flow\n",
    "w_cptr = WorkflowCheckpointer(workflow=w)\n",
    "\n",
    "# to checkpoint a run, use the `run` method from w_cptr\n",
    "handler = w_cptr.run(topic=\"Pirates\")\n",
    "await handler\n",
    "\n",
    "print ('here')\n",
    "# to view the stored checkpoints of this run\n",
    "for c in w_cptr.checkpoints[handler.run_id]:\n",
    "    print ('yo')\n",
    "    print (c)\n",
    "\n",
    "# to run from one of the checkpoints, use `run_from` method\n",
    "ckpt = w_cptr.checkpoints[handler.run_id][0]\n",
    "handler = w_cptr.run_from(topic=\"Ships\", checkpoint=ckpt)\n",
    "await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEvent(Event):\n",
    "    input: str\n",
    "\n",
    "\n",
    "class SetupEvent(Event):\n",
    "    error: bool\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CollectExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> SetupEvent:\n",
    "        # generically start everything up\n",
    "        if not hasattr(self, \"setup\") or not self.setup:\n",
    "            self.setup = True\n",
    "            print(\"I got set up\")\n",
    "        return SetupEvent(error=False)\n",
    "\n",
    "    @step\n",
    "    async def collect_input(self, ev: StartEvent) -> InputEvent:\n",
    "        if hasattr(ev, \"input\"):\n",
    "            # perhaps validate the input\n",
    "            print(\"I got some input\")\n",
    "            return InputEvent(input=ev.input)\n",
    "\n",
    "    @step\n",
    "    async def parse_query(self, ev: StartEvent) -> QueryEvent:\n",
    "        if hasattr(ev, \"query\"):\n",
    "            # parse the query in some way\n",
    "            print(\"I got a query\")\n",
    "            return QueryEvent(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def run_query(\n",
    "        self, ctx: Context, ev: InputEvent | SetupEvent | QueryEvent\n",
    "    ) -> StopEvent | None:\n",
    "        ready = ctx.collect_events(ev, [QueryEvent, InputEvent, SetupEvent])\n",
    "        if ready is None:\n",
    "            print(\"Not enough events yet\")\n",
    "            return None\n",
    "\n",
    "        # run the query\n",
    "        print(\"Now I have all the events\")\n",
    "        print(ready)\n",
    "\n",
    "        result = f\"Ran query '{ready[0].query}' on input '{ready[1].input}'\"\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got some input\n",
      "I got a query\n",
      "Not enough events yet\n",
      "Not enough events yet\n",
      "Now I have all the events\n",
      "[QueryEvent(query=\"Here's my question\"), InputEvent(input=\"Here's some input\"), SetupEvent(error=False)]\n",
      "Ran query 'Here's my question' on input 'Here's some input'\n"
     ]
    }
   ],
   "source": [
    "c = CollectExampleFlow()\n",
    "result = await c.run(input=\"Here's some input\", query=\"Here's my question\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent,\n",
    ")\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = FunctionAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant.\",\n",
    "    tools=[\n",
    "        FunctionTool.from_defaults(fn=add),\n",
    "        FunctionTool.from_defaults(fn=subtract),\n",
    "    ],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "retriever_agent = FunctionAgent(\n",
    "    name=\"retriever\",\n",
    "    description=\"Manages data retrieval\",\n",
    "    system_prompt=\"You are a retrieval assistant.\",\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[calculator_agent, retriever_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "\n",
    "#  Or stream the events\n",
    "handler = workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "async for event in handler.stream_events():\n",
    "    if hasattr(event, \"delta\"):\n",
    "        print(event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mReActOutputParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      ReAct Output parser.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/Github/100-days-of-grind/.venv/lib/python3.12/site-packages/llama_index/core/agent/react/output_parser.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "ReActOutputParser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
