{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsmidt/Library/Python/3.12/lib/python/site-packages/gradio/components/chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "import gradio as gr\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant. Please respond concisely.\")\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=8196,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    response = model.invoke(trimmed_messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "\n",
    "app.update_state(config, {\"messages\": [system_message]})\n",
    "\n",
    "def stream_response(message, history):\n",
    "    partial_message = \"\"\n",
    "    for chunk, _ in app.stream(\n",
    "        {\"messages\": [HumanMessage(message)]},\n",
    "        config,\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "            partial_message += chunk.content\n",
    "            yield partial_message\n",
    " \n",
    "gr.ChatInterface(stream_response).queue().launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        StateGraph\n",
      "\u001b[0;31mString form:\u001b[0m <langgraph.graph.state.StateGraph object at 0x11b7c5400>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/Library/Python/3.12/lib/python/site-packages/langgraph/graph/state.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "A graph whose nodes communicate by reading and writing to a shared state.\n",
      "The signature of each node is State -> Partial<State>.\n",
      "\n",
      "Each state key can optionally be annotated with a reducer function that\n",
      "will be used to aggregate the values of that key received from multiple nodes.\n",
      "The signature of a reducer function is (Value, Value) -> Value.\n",
      "\n",
      "Args:\n",
      "    state_schema (Type[Any]): The schema class that defines the state.\n",
      "    config_schema (Optional[Type[Any]]): The schema class that defines the configuration.\n",
      "        Use this to expose configurable parameters in your API.\n",
      "\n",
      "Examples:\n",
      "    >>> from langchain_core.runnables import RunnableConfig\n",
      "    >>> from typing_extensions import Annotated, TypedDict\n",
      "    >>> from langgraph.checkpoint.memory import MemorySaver\n",
      "    >>> from langgraph.graph import StateGraph\n",
      "    >>>\n",
      "    >>> def reducer(a: list, b: int | None) -> list:\n",
      "    ...     if b is not None:\n",
      "    ...         return a + [b]\n",
      "    ...     return a\n",
      "    >>>\n",
      "    >>> class State(TypedDict):\n",
      "    ...     x: Annotated[list, reducer]\n",
      "    >>>\n",
      "    >>> class ConfigSchema(TypedDict):\n",
      "    ...     r: float\n",
      "    >>>\n",
      "    >>> graph = StateGraph(State, config_schema=ConfigSchema)\n",
      "    >>>\n",
      "    >>> def node(state: State, config: RunnableConfig) -> dict:\n",
      "    ...     r = config[\"configurable\"].get(\"r\", 1.0)\n",
      "    ...     x = state[\"x\"][-1]\n",
      "    ...     next_value = x * r * (1 - x)\n",
      "    ...     return {\"x\": next_value}\n",
      "    >>>\n",
      "    >>> graph.add_node(\"A\", node)\n",
      "    >>> graph.set_entry_point(\"A\")\n",
      "    >>> graph.set_finish_point(\"A\")\n",
      "    >>> compiled = graph.compile()\n",
      "    >>>\n",
      "    >>> print(compiled.config_specs)\n",
      "    [ConfigurableFieldSpec(id='r', annotation=<class 'float'>, name=None, description=None, default=None, is_shared=False, dependencies=None)]\n",
      "    >>>\n",
      "    >>> step1 = compiled.invoke({\"x\": 0.5}, {\"configurable\": {\"r\": 3.0}})\n",
      "    >>> print(step1)\n",
      "    {'x': [0.5, 0.75]}"
     ]
    }
   ],
   "source": [
    "workflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'RunnableConfig'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[dict[str, Any], Any]]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mas_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'RunnableConfig'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Update the state of the graph with the given values, as if they came from\n",
      "node `as_node`. If `as_node` is not provided, it will be set to the last node\n",
      "that updated the state, if not ambiguous.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Library/Python/3.12/lib/python/site-packages/langgraph/pregel/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "app.update_state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
